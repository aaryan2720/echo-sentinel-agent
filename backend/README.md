# Echo Sentinel - Python Backend

FastAPI backend for deepfake detection using VideoMAE models.

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
# Navigate to backend folder
cd backend

# Activate virtual environment
.\venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
# Copy example env file
cp .env.example .env

# Edit .env and add your keys:
# - HUGGING_FACE_API_KEY (get from https://huggingface.co/settings/tokens)
# - VITE_SUPABASE_URL (copy from frontend .env)
# - VITE_SUPABASE_ANON_KEY (copy from frontend .env)
```

### 3. Run Server

```bash
# Development mode (auto-reload)
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Production mode
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

### 4. Test API

Open browser: **http://localhost:8000/docs**

You'll see interactive Swagger UI with all endpoints.

## ğŸ“¡ API Endpoints

### Health Check
```bash
GET http://localhost:8000/health
```

### Analyze Video (Upload)
```bash
POST http://localhost:8000/api/analyze/video
Content-Type: multipart/form-data

file: <video-file.mp4>
```

### Analyze Video (URL)
```bash
POST http://localhost:8000/api/analyze/video-url
Content-Type: application/json

{
  "url": "https://example.com/video.mp4"
}
```

### Get Models Info
```bash
GET http://localhost:8000/api/models
```

## ğŸ§ª Testing with cURL

### Test video upload:
```bash
curl -X POST "http://localhost:8000/api/analyze/video" \
  -F "file=@path/to/video.mp4"
```

### Test video URL:
```bash
curl -X POST "http://localhost:8000/api/analyze/video-url" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com/video.mp4"}'
```

## ğŸ¬ VideoMAE Model

This backend uses **VideoMAE** (Video Masked Autoencoders) for native video deepfake detection.

- **Model**: `shylhy/videomae-large-finetuned-deepfake-subset`
- **Accuracy**: 85-90%
- **Speed**: 2-5 seconds per video
- **Method**: Native video processing (not frame-by-frame)

### How it works:
1. Extracts 16 frames uniformly from video
2. Preprocesses frames using VideoMAEImageProcessor
3. Runs inference on GPU (if available) or CPU
4. Returns verdict (REAL/FAKE) with confidence score

## ğŸ—ï¸ Architecture

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py          # Package init
â”‚   â”œâ”€â”€ main.py              # FastAPI app, CORS, startup
â”‚   â”œâ”€â”€ config.py            # Settings from .env
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ analyze.py       # Analysis endpoints
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ video_analyzer.py # VideoMAE integration
â”œâ”€â”€ .env                      # Environment variables
â”œâ”€â”€ .env.example             # Example env file
â””â”€â”€ requirements.txt         # Python dependencies
```

## âš™ï¸ Configuration

Edit `.env` to configure:

- **API_HOST**: Server host (default: 0.0.0.0)
- **API_PORT**: Server port (default: 8000)
- **DEBUG**: Enable debug mode (default: True)
- **MAX_VIDEO_SIZE_MB**: Max upload size (default: 100MB)
- **USE_GPU**: Enable GPU acceleration (default: True)
- **FRONTEND_URL**: Frontend URL for CORS (default: http://localhost:8080)

## ğŸ”¥ GPU Acceleration

The backend automatically uses GPU if available:

```python
# Check GPU status
device = "cuda" if torch.cuda.is_available() else "cpu"
```

For NVIDIA GPUs, install CUDA version of PyTorch:
```bash
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

## ğŸ“Š Performance

- **Image Analysis**: ~2-3 seconds
- **Video Analysis**: ~2-5 seconds (16 frames)
- **GPU**: 10x faster than CPU
- **Memory**: ~2GB for model + ~1GB per video

## ğŸ› Troubleshooting

### Model not loading?
```bash
# Check Hugging Face access
python -c "from transformers import VideoMAEForVideoClassification; print('OK')"

# Download model manually
python -c "from transformers import VideoMAEForVideoClassification; VideoMAEForVideoClassification.from_pretrained('shylhy/videomae-large-finetuned-deepfake-subset')"
```

### CORS errors?
- Update `FRONTEND_URL` in `.env`
- Check `app/main.py` CORS configuration

### Out of memory?
- Reduce `MAX_VIDEO_SIZE_MB`
- Use CPU instead of GPU: `USE_GPU=False`
- Reduce number of workers

### Slow processing?
- Enable GPU: `USE_GPU=True`
- Install CUDA version of PyTorch
- Reduce video resolution before upload

## ğŸš€ Deployment

### Railway / Render / Fly.io

1. Add environment variables in dashboard
2. Set build command: `pip install -r requirements.txt`
3. Set start command: `uvicorn app.main:app --host 0.0.0.0 --port $PORT`

### Docker

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY app/ ./app/
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## ğŸ“š Documentation

See `docs/` folder:
- **PYTHON_BACKEND_ARCHITECTURE.md** - Complete architecture guide
- **PYTHON_PRODUCTION_ROADMAP.md** - Implementation roadmap
- **VIDEO_ANALYSIS_EXPLAINED.md** - How VideoMAE works

## ğŸ¯ Next Steps

1. âœ… Install dependencies
2. âœ… Configure `.env`
3. âœ… Run server
4. âœ… Test with Swagger UI
5. â³ Integrate with React frontend
6. â³ Deploy to production

## ğŸ’¡ Tips

- Use `/docs` for interactive API testing
- Check `/health` to verify models loaded
- Monitor logs with `loguru` (colored output)
- Test with small videos first (<10MB)
- GPU makes huge difference in speed

## ğŸ”— Links

- **VideoMAE Model**: https://huggingface.co/shylhy/videomae-large-finetuned-deepfake-subset
- **FastAPI Docs**: https://fastapi.tiangolo.com/
- **Hugging Face**: https://huggingface.co/

---

Built with â¤ï¸ for Echo Sentinel Agent
